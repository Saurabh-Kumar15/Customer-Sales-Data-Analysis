# ---------------------------------
# ðŸ“Š Multi-Faceted Customer & Sales Analysis
# ---------------------------------
# Author: [Your Name]
# Date: [Date]
# ---------------------------------

# ## 1. Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.mixture import GaussianMixture
from sklearn.cluster import AgglomerativeClustering

from prophet import Prophet
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score

from mlxtend.frequent_patterns import apriori, association_rules

# ## 2. Load Data
df = pd.read_csv('sample_data/sales_data.csv')

df['Date of Purchase'] = pd.to_datetime(df['Date of Purchase'])
df.head()

# ---------------------------------
# ## 3. Data Preprocessing
# ---------------------------------
# Handle missing values
num_cols = df.select_dtypes(include=np.number).columns
imputer = IterativeImputer()

# robust: re-wrap into DataFrame
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

num_cols = df.select_dtypes(include=np.number).columns

# drop problematic columns
clean_cols = [col for col in num_cols if df[col].nunique() > 1 and not df[col].isna().all()]
print(f"Columns used for imputation: {clean_cols}")

imputer = IterativeImputer()
imputed_data = imputer.fit_transform(df[clean_cols])
df[clean_cols] = pd.DataFrame(imputed_data, columns=clean_cols, index=df.index)



# Handle outliers with capping
for col in num_cols:
    q1, q3 = np.percentile(df[col], [25, 75])
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    df[col] = np.clip(df[col], lower, upper)

# Scale for clustering
scaler = StandardScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

# ---------------------------------
# ## 4. Feature Engineering
# ---------------------------------
today = pd.to_datetime('2025-07-01')
rfm = df.groupby('Customer ID').agg({
    'Date of Purchase': lambda x: (today - x.max()).days,
    'Purchase Quantity': 'count',
    'Unit Price': 'sum'
}).rename(columns={'Date of Purchase':'Recency','Purchase Quantity':'Frequency','Unit Price':'Monetary'})

rfm['CLV'] = rfm['Monetary'] * rfm['Frequency']
rfm['Recency Score'] = pd.qcut(rfm['Recency'], 4, labels=False)
rfm['Frequency Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 4, labels=False)
rfm.head()

# ---------------------------------
# ## 5. EDA
# ---------------------------------
sns.heatmap(df.select_dtypes(include=np.number).corr(), cmap='coolwarm')

plt.title('Correlation Matrix')
plt.show()

df['Month'] = df['Date of Purchase'].dt.to_period('M')
monthly_sales = df.groupby('Month').agg({'Unit Price':'sum'})
monthly_sales.plot(title='Monthly Sales')
plt.show()

# ---------------------------------
# ## 6. Customer Segmentation
# ---------------------------------
features = rfm[['Recency', 'Frequency', 'Monetary', 'CLV']]
minmax = MinMaxScaler()
features_scaled = minmax.fit_transform(features)

gmm = GaussianMixture(n_components=4, random_state=42)
rfm['Cluster'] = gmm.fit_predict(features_scaled)

agglo = AgglomerativeClustering(n_clusters=4)
rfm['Cluster_agglo'] = agglo.fit_predict(features_scaled)

sns.scatterplot(x='Recency', y='CLV', hue='Cluster', data=rfm, palette='Set1')
plt.title('Customer Segments')
plt.show()

# ---------------------------------
# ## 7. Sales Forecasting
# ---------------------------------
df_prophet = monthly_sales.reset_index()
df_prophet['Month'] = df_prophet['Month'].astype(str)
df_prophet.columns = ['ds', 'y']

model = Prophet()
model.fit(df_prophet)
future = model.make_future_dataframe(periods=6, freq='M')
forecast = model.predict(future)
fig = model.plot(forecast)
plt.show()

# ---------------------------------
# ## 8. Predictive Churn Modeling
# ---------------------------------
churn_df = rfm.copy()
churn_df['Churn'] = np.where(churn_df['Recency'] > churn_df['Recency'].quantile(0.75), 1, 0)

X = churn_df[['Recency', 'Frequency', 'Monetary', 'CLV']]
y = churn_df['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
xgb = XGBClassifier()
grid = GridSearchCV(xgb, {'max_depth':[3,5],'learning_rate':[0.01,0.1],'n_estimators':[100]}, cv=3)
grid.fit(X_train, y_train)
best_xgb = grid.best_estimator_

y_pred = best_xgb.predict(X_test)
print(classification_report(y_test, y_pred))
print("AUC-ROC:", roc_auc_score(y_test, best_xgb.predict_proba(X_test)[:,1]))

# ---------------------------------
# ## 9. Market Basket Analysis
# ---------------------------------
basket = (df.groupby(['Customer ID', 'Product ID'])['Purchase Quantity']
          .sum().unstack().fillna(0).applymap(lambda x: 1 if x>0 else 0))

freq_items = apriori(basket, min_support=0.05, use_colnames=True)
rules = association_rules(freq_items, metric='lift', min_threshold=1.1)
rules[['antecedents','consequents','support','confidence','lift']].head()

# ---------------------------------
# ## 10. Interactive Visualization
# ---------------------------------
fig = px.scatter(rfm, x='Recency', y='CLV', color='Cluster', title='Customer Segments Interactive')
fig.show()
